{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abdfe4e-dfd2-412a-bdf3-dae178e9b138",
   "metadata": {},
   "source": [
    "# Guidance Garage Demo\n",
    "\n",
    "This notebook is a brief introduction to the capabilities of [Guidance](https://github.com/guidance-ai/guidance), a Python package designed to make interfacing with LLMs both easier and more reliable. Guidance has support for a variety of LLMs, although its constrained generation features only work with local models (mostly).\n",
    "\n",
    "Before you begin, please install the required packages into your Python environment:\n",
    "\n",
    "```bash\n",
    "pip install -r /path/to/this/directory/requirements.txt\n",
    "```\n",
    "\n",
    "We will be using the 'mini' Phi-3 model in this demo, but it should work with most models available via Hugging Face Transformers. After installing the above packages, you shoulod be able to [run the sample inferencing code for Phi-3](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#sample-inference-code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150de73-78b5-4bce-9756-1571e2403143",
   "metadata": {},
   "source": [
    "## Simple Usage\n",
    "\n",
    "We will start by importing guidance and some useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90cf9f-3780-4f85-9e38-234fda08d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "from guidance import gen, select, models, assistant, system, user, with_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7d9ba-1aa3-4675-a164-a573b37110b1",
   "metadata": {},
   "source": [
    "Next, we create a `Model` object, which is Guidance's abstract representation of an LLM. We will use the `Transformers` implementation of `Model`, and load Phi-3 from the Hugging Face hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4c436-d96b-4574-8f4b-1c2c273e7033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm = models.Transformers(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362905c-fc78-468f-9bbe-e761f5f346be",
   "metadata": {},
   "source": [
    "`Model` objects are immutable, so each time we assign, we actually make a copy (the copy is shallow; we do not make a copy of the underlying LLM). As we accumulate prompts and responses, these are stored in the `Model` object so that we can reference them later.\n",
    "\n",
    "We can use the `user()` and `assistant()` context managers to build a conversation with the model. Sending prompts to the model is a matter of string concatenation, and we can get a response from the model by calling `gen()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521abe33-c507-42b6-8fc0-22c11f47badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_lm = lm\n",
    "\n",
    "with user():\n",
    "    chat_lm += \"How are you?\"\n",
    "\n",
    "with assistant():\n",
    "    chat_lm += gen(\"chat_response\", max_tokens=20, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d945a-d624-488b-b956-b93af09bb5a1",
   "metadata": {},
   "source": [
    "The first argument to `gen()` is a key we can use to extract the specific text generated by the call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea11b4-3152-484f-b6e3-d0fe8af98fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_lm[\"chat_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209a9c9-4e0e-45e2-9919-db82b1bfe6f1",
   "metadata": {},
   "source": [
    "## Constrained Generation\n",
    "\n",
    "Constrained generation is a powerful feature of Guidance. With it, we can force the model to produce an answer from a list we specify. It is accessed via the `select()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb81999-a5cf-4311-9449-6df4194971f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_lm = lm\n",
    "\n",
    "with user():\n",
    "    food_lm += \"Do you like brussels sprouts?\"\n",
    "\n",
    "with assistant():\n",
    "    food_lm += with_temperature(select(name=\"brussels\", options=[\"Yes, I like them\" , \"No I despise them\"]), temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898b5fc-23a3-49b8-9651-a68447b8f30f",
   "metadata": {},
   "source": [
    "The output here also shows some of the power of constrained generation: only the first token is highlighted, which means that only the first token was generated by the model. Once that was generated, Guidance was able to see that only one of the `options` passed to `select()` was still a possibility, and was therefore able to inject the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f0da5-a889-46f8-9aa5-f81834c29941",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_lm[\"brussels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9794c-e0f6-4aa7-98d7-1d725f485b28",
   "metadata": {},
   "source": [
    "## JSON Generation\n",
    "\n",
    "Constrained generation is really powerful when working with formatted data, such as JSON. Guidance can be taught any context-free grammar as a constraint, but it comes with (partial) support for [JSON schema](https://json-schema.org/). Let us start with some useful `import` statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfcbe3-0a2d-4aca-bf50-587d6a63e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonschema import validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe814cbe-4ac3-4978-a792-5374412e4b51",
   "metadata": {},
   "source": [
    "For our example, we're going to generate characters for a role-playing game, in the usual Tolkienesque setting. We can write a very simple JSON schema for these characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43742840-4431-45c2-858f-5444de5f91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_schema = \"\"\"{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"description\" : { \"type\" : \"string\" },\n",
    "        \"name\" : { \"type\" : \"string\" },\n",
    "        \"age\" : { \"type\" : \"integer\" },\n",
    "        \"armour\" : { \"type\" : \"string\", \"enum\" : [\"leather\", \"chainmail\", \"plate\"] },\n",
    "        \"weapon\" : { \"type\" : \"string\", \"enum\" : [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"] },\n",
    "        \"class\" : { \"type\" : \"string\" },\n",
    "        \"mantra\" : { \"type\" : \"string\" },\n",
    "        \"strength\" : { \"type\" : \"integer\" },\n",
    "        \"quest_items\" : { \"type\" : \"array\", \"items\" : { \"type\" : \"string\" } }\n",
    "    },\n",
    "    \"additionalProperties\": false\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "character_schema_obj = json.loads(character_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0fa8f-7f4e-4cb6-ab7c-ba64d97685ce",
   "metadata": {},
   "source": [
    "Now, let's import the necessary function from Guidance itself. Since we have already imported the json package, we have to rename it on import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723aaa7-c8aa-4cbb-a05d-24f8e907b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import json as gen_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44728ac-7ea7-4517-b8a6-825126c5921f",
   "metadata": {},
   "source": [
    "We can use this with a standard one-shot prompting strategy. We provide the LLM with a system prompt via the `system()` context manager, and then our one-shot example with the `user()` and `assistant()` context managers as before. We then put in our actual request, and call `gen_json()` with the schema object we just loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8c1d8-b9ab-4e37-b6fc-d611a343ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_lm = lm\n",
    "\n",
    "with system():\n",
    "    character_lm += \"\"\"You are a DM creating characters for a game in a Tolkienesque setting.\n",
    "Users will provide a one-line description of a character, you and should respond with a longer\n",
    "description in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "# Now give an example\n",
    "with user():\n",
    "    character_lm += \"A quick and nimble fighter\"\n",
    "\n",
    "with assistant():\n",
    "    character_lm += \"\"\"{\n",
    "    \"description\": \"A quick and nimble fighter\",\n",
    "    \"name\": \"Mokosh\",\n",
    "    \"age\": 20,\n",
    "    \"armour\": \"chainmail\",\n",
    "    \"weapon\": \"sword\",\n",
    "    \"class\": \"fighter\",\n",
    "    \"mantra\": \"I am the sword of the gods\",\n",
    "    \"strength\": 10,\n",
    "    \"quest_items\": [\n",
    "        \"Bag of holding\",\n",
    "        \"Amulet of Perun\",\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# Now ask for our character\n",
    "with user():\n",
    "    character_lm += \"A character attuned to the forest\"\n",
    "\n",
    "with assistant():\n",
    "    character_lm += gen_json(schema=character_schema_obj, name=\"next_character\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704929e-6f24-4f85-80e8-bcb0744a37dc",
   "metadata": {},
   "source": [
    "Notice how only a subset of tokens in the output were actually produced by the LLM. Many of the other tokens could be forced because the model was constrained by the schema.\n",
    "\n",
    "We can show that we really produced valid JSON with `json.loads()`, and also validate it against the schema we provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0b18a-4652-4fd2-9034-62c59ce772e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_character = json.loads(character_lm[\"next_character\"])\n",
    "\n",
    "validate(instance=loaded_character, schema=character_schema_obj)\n",
    "\n",
    "print(json.dumps(loaded_character, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bde50-a1e7-499a-9585-aa24f837401e",
   "metadata": {},
   "source": [
    "With Guidance, we can do even better. Using the `@guidance` decorator, we can create functions which can be used with the 'string concatenation' approach, much like `gen()` and `select()` (not to mention `json()` itself - used as `gen_json()` here). The function must accept a Guidance `Model` as its first argument, and then return a `Model` at the end. Inside, you can call other Guidance functions (or Python ones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30654b26-1d57-4cc6-b604-5efdd733bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance\n",
    "def generate_character(\n",
    "    lm_curr,\n",
    "    key: str,\n",
    "    character_one_liner: str,\n",
    "    temperature: float\n",
    "):\n",
    "    with system():\n",
    "        lm_curr += \"\"\"You are a DM creating characters for a game set in a Tolkienesque setting.\n",
    "Users will provide a one-line description of a character, you and should respond with a longer\n",
    "description in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "    # Now give an example\n",
    "    with user():\n",
    "        lm_curr += \"A quick and nimble fighter\"\n",
    "\n",
    "    with assistant():\n",
    "        lm_curr += \"\"\"{\n",
    "    \"description\": \"A quick and nimble fighter\",\n",
    "    \"name\": \"Mokosh\",\n",
    "    \"age\": 20,\n",
    "    \"armour\": \"chainmail\",\n",
    "    \"weapon\": \"sword\",\n",
    "    \"class\": \"fighter\",\n",
    "    \"mantra\": \"I am the sword of the gods\",\n",
    "    \"strength\": 10,\n",
    "    \"quest_items\": [\n",
    "        \"Bag of holding\",\n",
    "        \"Amulet of Perun\",\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "    # Now ask for our character\n",
    "    with user():\n",
    "        lm_curr += character_one_liner\n",
    "\n",
    "    with assistant():\n",
    "        lm_curr += gen_json(schema=character_schema_obj, name=key)\n",
    "\n",
    "    return lm_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ee865-a853-4bc7-95c5-0c579c81d6b9",
   "metadata": {},
   "source": [
    "To be a _stateless_ Guidance function, the supplied function must not reference any of its own generations (i.e. it mustn't contain code like `lm[\"my_generation\"]`), but that only matters when interacting with a remote endpoint.\n",
    "\n",
    "We can now use this function like `gen()` or `select()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888b3b9-d46c-46e0-baad-9a687e895d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"new_character\"\n",
    "char_0 = lm + generate_character(character_one_liner=\"A crafty rogue\", key=key, temperature=0.8)\n",
    "\n",
    "print(json.dumps(json.loads(char_0[key]), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c26aa-1699-4fb1-8963-269d4a4197d2",
   "metadata": {},
   "source": [
    "And again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a9267-ffb0-4598-9053-7146f4a0150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_1 = char_0 + generate_character(character_one_liner=\"A paladin from strange lands\", key=key, temperature=0.8)\n",
    "\n",
    "print(json.dumps(json.loads(char_1[key]), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542522d-7a34-48cf-a451-494769c714d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
